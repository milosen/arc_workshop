{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b0859a1-27fb-4496-b4fd-f87ab45ea567",
   "metadata": {},
   "source": [
    "# Controlled Stream Generation\n",
    "\n",
    "We will generate words and a lexicon with minimal feature overlap between words. Next, we introduce the 3 main ways to generate random streams based on a lexicon. Each specifies how the transition probabilities (TPs) of their syllables are structured:\n",
    "\n",
    "1. uniformlly distributed TPs, called \"TP-random position-random\" in the paper, \n",
    "2. position-controlled TPs, called \"TP-random position-fixed\", and\n",
    "3. TPs that fully preserve the words, called \"TP-structured\".\n",
    "\n",
    "## Syllable and Word Generation\n",
    "\n",
    "First, we generate/reload the words register (see arc types tutorial).\n",
    "\n",
    "Because ARC runs probabilistically (to speed things up), we set the random seeds to make sure our runs are reproducible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4869bfe5-e2e0-418d-b617-b0b5c08ce1e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import random\n",
    "import os\n",
    "\n",
    "np.random.seed(0)\n",
    "random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a6ccbb0-e9cf-4078-a87a-64763665b221",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load words...\n",
      "nyːfuːɡaː|huːpoːzɛː|løːvaːkuː|tiːfyːhøː|ɡiːʃoːmyː|køːvaːroː|tuːfoːheː|nuːkɛːfiː|hiːtyːvaː|hiːtyːfuː|... (1849 elements total)\n"
     ]
    }
   ],
   "source": [
    "from arc import load_words\n",
    "\n",
    "print(\"Load words...\")\n",
    "words = load_words(os.path.join(\"results\", \"words.json\"))\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06413261-d35b-4c93-b6da-ff39cc51805b",
   "metadata": {},
   "source": [
    "## Lexicon Generation\n",
    "\n",
    "Now we generate lexica with minimal feature repetitiveness. Let's start with 4 words each. \n",
    "\n",
    "By default, the function will generate 5 `Lexicon`s max. Let's generate 2 and print some info."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6491055a-9398-44dc-8b61-b457bf928f15",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Increasing allowed overlaps: MAX_PAIRWISE_OVERLAP=1, MAX_CUMULATIVE_OVERLAP=1\n",
      "Increasing allowed overlaps: MAX_PAIRWISE_OVERLAP=1, MAX_CUMULATIVE_OVERLAP=2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ɡaːfiːloː|hiːdoːfyː|foːreːɡiː|ʃuːkaːmɛː\n",
      "koːruːvaː|faːhøːdeː|ʃoːpeːhuː|nuːɡiːfoː\n"
     ]
    }
   ],
   "source": [
    "from arc import make_lexicons, load_words\n",
    "\n",
    "lexicons = make_lexicons(words, n_lexicons=2, n_words=4, control_features=True)\n",
    "print(\"\")\n",
    "\n",
    "for lexicon in lexicons:\n",
    "    print(lexicon)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0c5caf",
   "metadata": {},
   "source": [
    "By default, Lexicons with the minimum possible cumulative feature repetitiveness will be generated first, starting at zero. This means words will be joined into a lexicon if the features of all word pairs in the lexicon have no overlap. If it is not possible to generate the requested number Lexicons with zero overlap, the allowed overlap will be increased untill all lexicons are collected, which will be indicated by a warning message.\n",
    "\n",
    "This process will be repeated, until any of the following statements is true\n",
    "- the requested number of Lexicons has been generated\n",
    "- the maximum allowed overlap is reached (set via `max_overlap`)\n",
    "- the set of all word combinations is exhausted\n",
    "\n",
    "If one or more Lexicons is returned, their info fields hold the cumulative overlap between all word pairs that is achieved by the Lexicon as well as the maximum pairwise overlap used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eab5ca7e-5929-446b-8a9e-78c979d3b34d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lexicon: ɡaːfiːloː|hiːdoːfyː|foːreːɡiː|ʃuːkaːmɛː\n",
      "cumulative_feature_repetitiveness: 2\n",
      "max_pairwise_feature_repetitiveness: 1\n",
      "\n",
      "Lexicon: koːruːvaː|faːhøːdeː|ʃoːpeːhuː|nuːɡiːfoː\n",
      "cumulative_feature_repetitiveness: 2\n",
      "max_pairwise_feature_repetitiveness: 1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for lexicon in lexicons:\n",
    "    print(\"Lexicon:\", lexicon)\n",
    "    print(\"cumulative_feature_repetitiveness:\", lexicon.info[\"cumulative_feature_repetitiveness\"])\n",
    "    print(\"max_pairwise_feature_repetitiveness:\", lexicon.info[\"max_pairwise_feature_repetitiveness\"])\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9186f7db",
   "metadata": {},
   "source": [
    "## Stream Generation\n",
    "\n",
    "We want to generate a complete set of compatible lexicons for our study, i.e. to generate a compatible set of streams for testing statistical learning hypotheses. If `streams` is empty, try increasing the allowed maximum rythmicity.\n",
    "\n",
    "The function `make_streams` will try to generate one stream for each lexicon and TP mode. If you specify 'max_rhythmicity', it will discard those that do not meet the requirement. By default, all streams from a lexicon will be discarded if the lexicon can't generate streams for all requested TP modes. Printed below you see a collection of streams. Because streams can get long, you only see their key consisting of the lexicon used to generate it and its TP mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "82199da6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Increasing allowed overlaps: MAX_PAIRWISE_OVERLAP=1, MAX_CUMULATIVE_OVERLAP=1\n",
      "Increasing allowed overlaps: MAX_PAIRWISE_OVERLAP=1, MAX_CUMULATIVE_OVERLAP=2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lexicon-lyːfoːkuː-huːʃiːbaː-faːhiːtuː-koːvaːniː_TP-random|Lexicon-lyːfoːkuː-huːʃiːbaː-faːhiːtuː-koːvaːniː_TP-word_structured|Lexicon-lyːfoːkuː-huːʃiːbaː-faːhiːtuː-koːvaːniː_TP-position_controlled\n"
     ]
    }
   ],
   "source": [
    "from arc import make_streams, make_lexicons, load_words\n",
    "\n",
    "lexicons = make_lexicons(words, n_lexicons=1, n_words=4, control_features=True)\n",
    "streams = make_streams(lexicons, require_all_tp_modes=True)\n",
    "\n",
    "print(streams)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac301ad8-f883-4de7-9719-7db125b37570",
   "metadata": {},
   "source": [
    "To inspect a stream, you just have to select one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d224f751-3cc9-4549-b6e7-b3697f11b93b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kuːbaːlyːhiːtuːfaːkoːniːvaːhuːfoːʃiːfaːhuːtuːfoːlyːniːbaːkuːkoːʃiːhiːvaːniːfaːkuːvaːʃiːfoːtuːkoːhiːlyːhuːbaːtuːvaːfoːkoːkuːhuːniːʃiːlyːfaːhiːbaːfoːfaːniːhiːkoːhuːvaːbaːʃiːkuːtuːlyːbaːvaːkuːʃiːniːtuːhuːlyːkoːfaːfoːhiːkuːlyːtuːhiːfaːbaːkoːfoːniːhuːʃiːvaːlyːvaːfaːʃiːbaːhiːniːkoːtuːkuːfoːhuːhiːhuːkuːfaːtuːʃiːkoːbaːniːlyːfoːvaːtuːniːfoːbaːfaːlyːʃiːhuːkoːvaːhiːkuːniːkuːhiːfoːlyːfaːvaːkoːʃiːtuːbaːhuːfaːhiːʃiːlyːkuːkoːbaːniːfoːhuːvaːtuːfoːkuːbaːvaːʃiːfaːkoːlyːhuːhiːtuːniːbaːtuːlyːkuːhuːkoːniːʃiːfoːvaːhiːfaːlyːkoːhiːhuːniːkuːtuːfaːvaːfoːʃiːbaːʃiːvaːkoːfoːtuːbaːhuːfaːkuːhiːlyːniːtuːhuːfoːniːkoːvaːfaːbaːhiːʃiːkuːlyːbaːfoːfaːniːvaːhuːkuːʃiːtuːkoːlyːhiːvaːniːfaːhuːtuːʃiːkoːkuːfoːhiːbaːlyːtuːhiːniːlyːfoːkoːfaːʃiːhuːbaːkuːvaːlyːvaːbaːkoːhuːʃiːniːhiːfoːkuːfaːtuːvaːkuːniːhuːlyːʃiːhiːkoːtuːfoːbaːfaːfoːniːkuːʃiːhiːkoːfaːhuːlyːbaːvaːtuːkuːfoːhuːʃiːtuːvaːlyːfaːhiːniːkoːbaːtuːniːfaːkuːbaːhuːfoːkoːhiːlyːʃiːvaːbaːfoːkuːfaːtuːlyːhuːhiːʃiːkoːvaːniːhiːfaːkoːtuːhuːbaːʃiːniːfoːvaːkuːlyːfoːlyːkuːkoːhuːvaːʃiːfaːniːtuːbaːhiːbaːlyːkoːniːvaːfaːfoːʃiːhuːkuːhiːtuːhiːfoːbaːfaːvaːhuːkoːkuːtuːʃiːlyːniːhuːniːʃiːbaːkoːfoːtuːfaːlyːvaːhiːkuːniːbaːkuːhuːtuːkoːlyːhiːvaːfoːfaːʃiːkuːvaːkoːʃiːfoːhiːhuːfaːbaːniːlyːtuːkuːtuːfaːlyːhuːfoːhiːkoːvaːbaːniːʃiːfaːʃiːniːvaːkoːbaːhiːhuːlyːtuːkuːfoːvaːtuːhiːlyːʃiːkuːkoːfoːhuːfaːniːbaːʃiːbaːfaːtuːlyːhiːkuːvaːfoːkoːniːhuːvaːkuːfaːfoːbaːkoːlyːniːtuːhuːʃiːhiːfaːhuːniːhiːfoːlyːbaːtuːvaːʃiːkoːkuːniːkoːʃiːlyːvaːfaːbaːkuːhuːhiːtuːfoːtuːʃiːvaːhuːbaːlyːkuːhiːniːfoːfaːkoːfaːhiːbaːvaːlyːfoːniːkuːʃiːhuːtuːkoːtuːbaːhuːkoːhiːʃiːfoːkuːlyːfaːvaːniːlyːkoːhuːkuːbaːfoːʃiːtuːniːfaːhiːvaːhiːfoːtuːvaːhuːkuːkoːlyːbaːniːʃiːfaːkuːniːtuːbaːhuːhiːkoːfoːʃiːlyːfaːvaːʃiːvaːkuːtuːniːkoːbaːlyːhiːhuːfoːfaːniːfaːtuːfoːhuːlyːvaːkoːkuːhiːʃiːbaː\n"
     ]
    }
   ],
   "source": [
    "stream = streams[0]\n",
    "print(stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "65e606a1-8427-4d09-9058-f2c0e330b275",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP mode: random\n",
      "Lexicon: lyːfoːkuː|huːʃiːbaː|faːhiːtuː|koːvaːniː\n",
      "Feature PRIs:\n",
      "  phon_1_son 0.06315789473684211\n",
      "  phon_1_back 0.03508771929824561\n",
      "  phon_1_hi 0.03508771929824561\n",
      "  phon_1_lab 0.07192982456140351\n",
      "  phon_1_cor 0.09298245614035087\n",
      "  phon_1_cont 0.015789473684210527\n",
      "  phon_1_lat 0.0\n",
      "  phon_1_nas 0.0\n",
      "  phon_1_voi 0.09298245614035087\n",
      "  phon_2_back 0.005263157894736842\n",
      "  phon_2_hi 0.010526315789473684\n",
      "  phon_2_lo 0.03684210526315789\n",
      "  phon_2_lab 0.05087719298245614\n",
      "  phon_2_tense 0.0\n",
      "  phon_2_long 0.0\n"
     ]
    }
   ],
   "source": [
    "print(\"TP mode:\", stream.info[\"stream_tp_mode\"])\n",
    "print(\"Lexicon:\", stream.info[\"lexicon\"])\n",
    "print(\"Feature PRIs:\") \n",
    "for feat, pri in stream.info[\"rhythmicity_indexes\"].items():\n",
    "    print(\" \", feat, pri)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7558308-40fa-4fb5-8ec1-585fc73ba250",
   "metadata": {},
   "source": [
    "As you can see, the `.info` field holds some useful information about the generated stream, i.e. which Lexicon has been used to generate it, the rythmicity indexes achieved for each feature, and which randomization/TP-structure mode has been used.\n",
    "\n",
    "This concludes the second tutorial, and we end this series with the third and last tutorial about how to use your own data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "818242aa-4b28-4dcd-a035-85099c2bc1a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
